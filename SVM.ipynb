{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82c9e9e",
   "metadata": {},
   "source": [
    "Mathematical Formula for a Linear SVM:\n",
    "\n",
    "The mathematical formula for a linear SVM is given by the equation of the hyperplane:\n",
    "\n",
    "f(x)=w⋅x+b\n",
    "where:\n",
    "\n",
    "f(x) represents the decision function.\n",
    "w\n",
    "w is the weight vector perpendicular to the hyperplane.\n",
    "x\n",
    "x is the input feature vector.\n",
    "\n",
    "b is the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fc19c2",
   "metadata": {},
   "source": [
    "<!-- Objective Function of a Linear SVM:\n",
    "\n",
    "The objective function of a linear SVM aims to maximize the margin between the support vectors and the decision boundary while minimizing the classification error. It can be formulated as:\n",
    "minimize \n",
    "1\n",
    "2\n",
    "∥\n",
    "w\n",
    "∥\n",
    "2\n",
    "+\n",
    "\n",
    "minimize  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " +C∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ξ \n",
    "i\n",
    "​\n",
    " \n",
    "subject to:\n",
    "\n",
    ",\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)≥1−ξ \n",
    "i\n",
    "\n",
    " ≥0\n",
    "where:\n",
    "w\n",
    "w is the weight vector.\n",
    "\n",
    "C is the regularization parameter controlling the trade-off between maximizing the margin and minimizing the classification error. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba471c9",
   "metadata": {},
   "source": [
    "Kernel Trick in SVM:\n",
    "\n",
    "The kernel trick in SVM allows for nonlinear decision boundaries by implicitly mapping the input features into a higher-dimensional space where a linear decision boundary can separate the classes. This is done by replacing the dot product \n",
    "\n",
    "x \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    "  in the objective function with the kernel function \n",
    "\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ), which computes the similarity between two feature vectors in the original input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb7dc0",
   "metadata": {},
   "source": [
    "Role of Support Vectors in SVM:\n",
    "\n",
    "Support vectors are the data points that lie closest to the decision boundary (margin). They play a crucial role in determining the position of the decision boundary since they define the margin. In essence, the decision boundary is determined by the support vectors, and all other data points are irrelevant for defining the decision boundary. For example, in a binary classification problem, support vectors are the data points from both classes that are closest to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1babf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
